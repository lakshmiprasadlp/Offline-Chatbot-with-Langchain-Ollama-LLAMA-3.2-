

# ğŸ¤– Offline Chatbot with LangChain, Ollama, and LLama 3.2

This project demonstrates how to build a fully **offline AI chatbot** using **LangChain**, **Ollama**, and **LLama 3.2**. The chatbot runs entirely on your local machine, ensuring **data privacy**, **low latency**, and full **control over the LLM stack**â€”without relying on internet access or external APIs.

---

## ğŸš€ Features

* **Fully Offline**: No internet or cloud API requiredâ€”works even in air-gapped environments.
* **Privacy-Preserving**: All data stays local. No conversations are logged or shared.
* **Powered by LLama 3.2**: High-performance open-source language model from Meta.
* **LangChain Integration**: Modular, extensible architecture using LangChain components.
* **Customizable**: Add your own tools, retrievers, and prompts easily.

---
# Screan Short
![Chatbot Output](assets/Screenshot 1.png)
![Chatbot Output](assets/Screenshot 1.png)

---
## âœ… Advantages

| Advantage                 | Description                                                                |
| ------------------------- | -------------------------------------------------------------------------- |
| ğŸ” **Offline Capability** | No dependency on OpenAI, Anthropic, or any external services.              |
| ğŸ›¡ï¸ **No Data Leakage**   | All interactions and data are processed locallyâ€”ensuring maximum security. |
| ğŸš€ **Low Latency**        | Fast responses as thereâ€™s no network overhead.                             |
| ğŸ§© **Modular Design**     | Easily extend with custom tools, memory, or retrievers via LangChain.      |
| ğŸ”„ **Repeatable Setup**   | Runs reliably across systems with reproducible environments.               |

---

## âš ï¸ Disadvantages

| Disadvantage                   | Description                                                                  |
| ------------------------------ | ---------------------------------------------------------------------------- |
| ğŸ–¥ï¸ **High Computation Needs** | Requires a system with a modern GPU and at least 8â€“16GB of VRAM.             |
| ğŸ§  **Model Size**              | LLama 3.2 is resource-intensive and may not run smoothly on low-end systems. |
| ğŸ• **Longer Setup Time**       | Initial download and setup of model weights can be time-consuming.           |

---

## ğŸ› ï¸ Tech Stack

* **LLM**: LLama 3.2 via [Ollama](https://ollama.com/)
* **Framework**: [LangChain](https://www.langchain.com/)
* **UI/UX**: Streamlit 
* **Local Execution**: No cloud or external API integration

---

## ğŸ“‚ Project Structure

```
offline-chatbot/
â”‚
â”œâ”€â”€ app.py                # Main chatbot script (LangChain/Ollama interface)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project overview
```

---

## ğŸ“¦ Installation

1. **Install Ollama** and run:

   ```bash
   ollama run llama3
   ```

2. **Clone the repo**:

   ```bash
   git clonehttps://github.com/lakshmiprasadlp/Offline-Chatbot-with-Langchain-Ollama-LLAMA-3.2-.git
   cd Offline-Chatbot-with-Langchain-Ollama-LLAMA-3.2-
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Run the chatbot**:

   ```bash
   python app.py
   ```

---

## ğŸ“Œ Use Cases

* Private enterprise chatbots
* Secure legal/medical advisors
* Offline field assistants (e.g., defense, rural, or research environments)

---

## ğŸ“ƒ License

This project is open-sourced under the **MIT License**. Feel free to modify and adapt it for your use.

---

